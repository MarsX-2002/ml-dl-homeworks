{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33642500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febbcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt').read().splitlines()\n",
    "\n",
    "vocab = sorted(set(''.join(names) + '.'))\n",
    "stoi = {v:k for k, v in enumerate(vocab)}\n",
    "itos = {v:k for k, v in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad96f9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikey/Desktop/maab_ml/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683b3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train neural bigram model\n",
    "vocab_size = len(stoi)  # 27\n",
    "batch_size = 512\n",
    "epochs = 20\n",
    "\n",
    "test_size = int(0.2 * len(names))\n",
    "\n",
    "tr_names = names[:-test_size]\n",
    "ts_names = names[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d405d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(names_list, stoi, context_size):\n",
    "    X, Y = [], []\n",
    "    for name in names_list:\n",
    "        name = '.' * context_size + name + '.'\n",
    "        for i in range(len(name) - context_size):\n",
    "            context = [stoi[c] for c in name[i:i+context_size]]\n",
    "            target = stoi[name[i+context_size]]\n",
    "            X.append(context)\n",
    "            Y.append(target)\n",
    "    return torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765c5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class NeuralNGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embed_dim=50, hidden_dim=200, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20138a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train_ngram(context_size=1, embed_dim=64, hidden_dim=128, lr=1e-3, epochs=15):\n",
    "    print(f\"\\nTraining {context_size}-gram model...\")\n",
    "    Xtr, Ytr = build_dataset(tr_names, stoi, context_size)\n",
    "    Xts, Yts = build_dataset(ts_names, stoi, context_size)\n",
    "\n",
    "    model = NeuralNGramModel(len(stoi), context_size, embed_dim, hidden_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    Xtr, Ytr, Xts, Yts = Xtr.to(device), Ytr.to(device), Xts.to(device), Yts.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        logits = model(Xtr)\n",
    "        loss = loss_fn(logits, Ytr)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = loss_fn(model(Xts), Yts).item()\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:2d} | train {loss.item():.4f} | test {val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89585d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference\n",
    "@torch.no_grad()\n",
    "def sample(model, context_size=1, num=5):\n",
    "    for _ in range(num):\n",
    "        context = [stoi['.']] * context_size\n",
    "        out = []\n",
    "        while True:\n",
    "            x = torch.tensor([context[-context_size:]], device=device)\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, 1).item()\n",
    "            ch = itos[ix]\n",
    "            if ch == '.': break\n",
    "            out.append(ch)\n",
    "            context.append(ix)\n",
    "        print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00d16476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 1-gram model...\n",
      "Epoch  1 | train 3.3183 | test 3.2032\n",
      "Epoch  5 | train 2.9080 | test 2.9154\n",
      "Epoch 10 | train 2.6550 | test 2.7879\n",
      "Epoch 15 | train 2.5694 | test 2.7275\n",
      "Epoch 20 | train 2.5267 | test 2.6743\n",
      "Epoch 25 | train 2.4982 | test 2.6415\n",
      "Epoch 30 | train 2.4842 | test 2.6294\n",
      "Epoch 35 | train 2.4724 | test 2.6191\n",
      "Epoch 40 | train 2.4680 | test 2.6123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNGramModel(\n",
       "  (embed): Embedding(27, 64)\n",
       "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = train_ngram(context_size=1, embed_dim=64, hidden_dim=128, lr=3e-3, epochs=40)\n",
    "bigram_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59615c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 2-gram model...\n",
      "Epoch  1 | train 3.3182 | test 3.1791\n",
      "Epoch  5 | train 2.8053 | test 2.8610\n",
      "Epoch 10 | train 2.5979 | test 2.7613\n",
      "Epoch 15 | train 2.5044 | test 2.6738\n",
      "Epoch 20 | train 2.4428 | test 2.6076\n",
      "Epoch 25 | train 2.3977 | test 2.5690\n",
      "Epoch 30 | train 2.3649 | test 2.5386\n",
      "Epoch 35 | train 2.3413 | test 2.5142\n",
      "Epoch 40 | train 2.3231 | test 2.5020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNGramModel(\n",
       "  (embed): Embedding(27, 64)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=128, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = train_ngram(context_size=2, embed_dim=64, hidden_dim=128, lr=3e-3, epochs=40)\n",
    "bigram_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e99e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
